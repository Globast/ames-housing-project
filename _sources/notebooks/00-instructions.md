---
jupytext:
  formats: md:myst
  text_representation: {extension: .md, format_name: myst}
kernelspec: {name: python3, display_name: Python 3}
---

# Cap√≠tulo 0 ¬∑ Instrucciones de reproducci√≥n
## Instrucciones de reproducci√≥n y Demostraciones
> **Overview**: Este capitulo presenta instrucciones de reproducci√≥n. Se muestran las demostraciones solicitadas.


## C√≥mo compilar el libro:

```bash
jupyter-book build .
```

**Dependencias**: ver `requirements.txt` en la ra√≠z de `book/`.

**Dataset**: se espera en `data/ames_housing.csv`.
- Descarga manual desde Kaggle o usa la API:
  ```bash
  kaggle datasets download -d prevek18/ames-housing-dataset -p data/ --unzip
  mv data/AmesHousing.csv data/ames_housing.csv
  ```

**Control de versiones** 
```
book/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ notebooks/
‚îú‚îÄ‚îÄ _build/
‚îî‚îÄ‚îÄ _config.yml
```

**Semillas reproducibles**: todos los experimentos fijan `random_state`.
**Mapa del libro**: ver la barra lateral y `_toc.yml`.



## Demostraciones solicitadas

### Enunciado


# Distribuci√≥n de la Suma de Cuadrados de los Residuos (OLS)

---

## Modelo y notaci√≥n

El modelo lineal es:

$$
\mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}, 
\qquad 
\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2 I_n)
$$

donde:

- \( $$X$$ \): matriz de dise√±o \( n \times k \)  
- \( $$\beta$$\ ): vector de par√°metros  
- \( $${\varepsilon}$$ ): vector de errores  

---

## Estimador OLS y matriz *hat*

El estimador de m√≠nimos cuadrados es:

$$
\hat{\boldsymbol{\beta}} = (X'X)^{-1} X' \mathbf{y}
$$

Las predicciones se obtienen con:

$$
\hat{\mathbf{y}} = X\hat{\boldsymbol{\beta}} = H\mathbf{y},
\quad 
H = X(X'X)^{-1}X'
$$

La matriz \( $$H$$\) (llamada hat matrix) proyecta \($$\mathbf{y} $$\) sobre el espacio columna de \( $$X$$ \).

*Propiedades de \( H \):*
- Sim√©trica: \( H' = H \)
- Idempotente: \( H^2 = H \)
- Rango: \( \operatorname{rank}(H) = k \)

---

## 3Ô∏è‚É£ Residuos y operador \( I - H \)

Los residuos son:

$$
\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = (I - H)\mathbf{y}
$$

Sustituyendo \( \mathbf{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon} \) y usando que \( (I - H)X = 0 \):

$$
\boxed{\mathbf{e} = (I - H)\boldsymbol{\varepsilon}}
$$

üëâ Cada residuo es una *combinaci√≥n lineal* de los errores originales.

---

## 4Ô∏è‚É£ Suma de cuadrados residual como forma cuadr√°tica

$$
SS_{\text{Res}} 
= \mathbf{e}'\mathbf{e} 
= [(I-H)\boldsymbol{\varepsilon}]'[(I-H)\boldsymbol{\varepsilon}]
= \boldsymbol{\varepsilon}'(I-H)'(I-H)\boldsymbol{\varepsilon}
$$

Como \( I - H \) es sim√©trica e idempotente:

$$
\boxed{SS_{\text{Res}} = \boldsymbol{\varepsilon}'(I - H)\boldsymbol{\varepsilon}}
$$

Es decir, el *SSR* es una forma cuadr√°tica en los errores.

---

## 5Ô∏è‚É£ Rango de \( I - H \) y grados de libertad

El rango de \( I - H \) se obtiene de:

$$
\operatorname{rank}(I - H) = n - \operatorname{rank}(H) = n - k
$$

Por tanto:
- El espacio de los residuos tiene *dimensi√≥n \( n - k \)*.  
- Solo \( n - k \) residuos son independientes (los otros est√°n restringidos por \( X'e = 0 \)).

En un modelo lineal simple (\( k = 2 \)) ‚Üí grados de libertad: *\( n - 2 \)*.

---

## 6Ô∏è‚É£ Distribuci√≥n Chi-cuadrado

*Resultado general:*

> Si \( \boldsymbol{\varepsilon} \sim N(0, \sigma^2 I) \) y \( A \) es sim√©trica e idempotente de rango \( r \):  
> $$
> \frac{1}{\sigma^2} \boldsymbol{\varepsilon}'A\boldsymbol{\varepsilon} \sim \chi^2_r
> $$

Aplicando con \( A = I - H \) (rango \( n - k \)):

$$
\boxed{
\frac{SS_{\text{Res}}}{\sigma^2}
= 
\frac{\boldsymbol{\varepsilon}'(I - H)\boldsymbol{\varepsilon}}{\sigma^2}
\sim 
\chi^2_{\,n-k}
}
$$

---

## 7Ô∏è‚É£ Por qu√© no basta con elevar cada residuo al cuadrado

- Los errores \( \varepsilon_i \) son independientes normales ‚áí  
  \( (\varepsilon_i / \sigma)^2 \sim \chi^2_1 \).
- Pero los *residuos* \( e_i \) no son iguales a \( \varepsilon_i \):  
  son combinaciones lineales ‚áí *no independientes*.
- Adem√°s, \( \operatorname{Var}(e_i) = \sigma^2 (1 - h_{ii}) \), donde \( h_{ii} \) es el leverage.  
- Por eso, la *suma total de cuadrados* \( e'e \) sigue una chi-cuadrado, no cada residuo individual.

---

## 8Ô∏è‚É£ Intuici√≥n geom√©trica

- \( \boldsymbol{\varepsilon} \) vive en un espacio de dimensi√≥n \( n \).  
- \( H \): proyecci√≥n sobre el subespacio de las predicciones (dimensi√≥n \( k \)).  
- \( I - H \): proyecci√≥n sobre el *espacio ortogonal de los residuos* (dimensi√≥n \( n - k \)).  
- La longitud al cuadrado de esa proyecci√≥n, dividida por \( \sigma^2 \), sigue una \( \chi^2_{n-k} \).

---

## ‚úÖ Resultado final

$$
\boxed{
SS_{\text{Res}} = \boldsymbol{\varepsilon}'(I - H)\boldsymbol{\varepsilon},
\quad
\frac{SS_{\text{Res}}}{\sigma^2} \sim \chi^2_{\,n-k}
}
$$

En regresi√≥n lineal simple:  
$$
k = 2 \Rightarrow n - 2 \text{ grados de libertad.}
$$






1. Sea un modelo de regresi√≥n lineal simple; muestra que la suma de cuadrados de los residuos dividida por $ \sigma^2 $ puede escribirse como una **combinaci√≥n cuadr√°tica** de los errores $ \varepsilon_i $ y, usando ese resultado, que su distribuci√≥n es $ \chi^2_{n-2} $.

La varianza es $ \sigma^2 $.

La esperanza es \( \mathbb{E}[X] \).

1. ‚Ä¶ dividida por $$ \sigma^2 $$ ‚Ä¶ los errores $ \varepsilon_i $$ ‚Ä¶ es $ \chi^2_{n-2} $.

### Paso 1 ‚Äî Modelo y notaci√≥n matricial

```{math}
:label: eq:2.11.1-modelo
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}, \qquad
\boldsymbol{\varepsilon} \sim \mathcal{N}(\mathbf{0},\, \sigma^2 \mathbf{I}_n)
```
donde en regresi√≥n simple
$\mathbf{X} = [\mathbf{1}\ \ \mathbf{x}] \in \mathbb{R}^{n\times 2}$
(tiene **p = 2** columnas: intercepto y regresor).

El estimador OLS es
```{math}
:label: eq:2.11.1-beta
\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}.
```
El vector de **residuos** es
```{math}
:label: eq:2.11.1-residuos
\mathbf{e} = \mathbf{y} - \hat{\mathbf{y}} = \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}
= \left(\mathbf{I}_n - \mathbf{H}\right)\mathbf{y},
```
con la **matriz sombrero** (proyecci√≥n) definida por
```{math}
:label: eq:2.11.1-hat
\mathbf{H} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top, \qquad
\mathbf{M} \equiv \mathbf{I}_n - \mathbf{H}.
```

### Paso 2 ‚Äî Forma cuadr√°tica

Sustituyendo {eq}`eq:2.11.1-modelo` en {eq}`eq:2.11.1-residuos`,
```{math}
:label: eq:2.11.1-e-M-eps
\mathbf{e} = \mathbf{M}\mathbf{y} = \mathbf{M}(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon})
= \underbrace{\mathbf{M}\mathbf{X}}_{=\ \mathbf{0}} \boldsymbol{\beta} + \mathbf{M}\boldsymbol{\varepsilon}
= \mathbf{M}\boldsymbol{\varepsilon},
```
pues $\mathbf{M}\mathbf{X} = (\mathbf{I}-\mathbf{H})\mathbf{X} = \mathbf{X} - \mathbf{X} = \mathbf{0}$.

La **suma de cuadrados de residuos** es
```{math}
:label: eq:2.11.1-ssr
SS_{\text{Res}} = \mathbf{e}^\top \mathbf{e} = (\mathbf{M}\boldsymbol{\varepsilon})^\top(\mathbf{M}\boldsymbol{\varepsilon})
= \boldsymbol{\varepsilon}^\top \mathbf{M}^\top \mathbf{M}\,\boldsymbol{\varepsilon}.
```
Como $\mathbf{H}$ es sim√©trica e idempotente ($\mathbf{H}=\mathbf{H}^\top$, $\mathbf{H}^2=\mathbf{H}$),
entonces $\mathbf{M}=\mathbf{I}-\mathbf{H}$ es tambi√©n **sim√©trica** e **idempotente**:
$\mathbf{M}^\top=\mathbf{M}$ y $\mathbf{M}^2=\mathbf{M}$.
Por tanto,
```{math}
:label: eq:2.11.1-ssr-forma
SS_{\text{Res}} = \boldsymbol{\varepsilon}^\top \mathbf{M}\,\boldsymbol{\varepsilon}
\qquad\Longrightarrow\qquad
\frac{SS_{\text{Res}}}{\sigma^2} = \left(\frac{\boldsymbol{\varepsilon}}{\sigma}\right)^\top
\mathbf{M}\left(\frac{\boldsymbol{\varepsilon}}{\sigma}\right).
```
Esta es una **combinaci√≥n cuadr√°tica** de los errores, como se ped√≠a.

### Paso 3 ‚Äî Distribuci√≥n $\chi^2$ y grados de libertad

Sea $\mathbf{z} \equiv \boldsymbol{\varepsilon}/\sigma \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_n)$.
Entonces, por {eq}`eq:2.11.1-ssr-forma`,
```{math}
:label: eq:2.11.1-qf
\frac{SS_{\text{Res}}}{\sigma^2} = \mathbf{z}^\top \mathbf{M}\,\mathbf{z}.
```
Teorema cl√°sico: si $\mathbf{A}$ es sim√©trica e idempotente con **rango $r$**, y $\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})$,
entonces $\mathbf{z}^\top \mathbf{A}\,\mathbf{z} \sim \chi^2_r$. Aqu√≠ $\mathbf{A}=\mathbf{M}$.

En regresi√≥n simple ($p=2$), $\operatorname{rank}(\mathbf{H})=p=2$, por lo que
```{math}
:label: eq:2.11.1-rank
\operatorname{rank}(\mathbf{M}) = \operatorname{rank}(\mathbf{I}-\mathbf{H}) = n - \operatorname{rank}(\mathbf{H}) = n-2.
```
Concluimos que
```{math}
:label: eq:2.11.1-chi2
\frac{SS_{\text{Res}}}{\sigma^2} \sim \chi^2_{\,n-2}.
```

### Comentario final (intuici√≥n)

- $\mathbf{H}$ proyecta $\mathbf{y}$ sobre el subespacio generado por las columnas de $\mathbf{X}$ (dimensi√≥n $p$).  
- $\mathbf{M}$ proyecta sobre su **complemento ortogonal** (dimensi√≥n $n-p$), donde viven los residuos.  
- Al estandarizar los errores por $\sigma$, la energ√≠a de la proyecci√≥n en ese subespacio (forma cuadr√°tica) sigue una $\chi^2$ con $n-p$ grados de libertad, que aqu√≠ es $n-2$.

**Referencias internas.** De {eq}`eq:2.11.1-e-M-eps` y {eq}`eq:2.11.1-ssr` se obtiene la forma cuadr√°tica {eq}`eq:2.11.1-ssr-forma`; usando {eq}`eq:2.11.1-rank` se deduce la distribuci√≥n en {eq}`eq:2.11.1-chi2`.

